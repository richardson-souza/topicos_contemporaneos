{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixa e extrai o dataset\n",
    "!mkdir data/\n",
    "!curl -L -o data/animais.zip \"https://drive.google.com/uc?export=download&id=16Lll0Slg1unWxAb26AzZqI9sPdB_fYpV\"\n",
    "!unzip data/animais.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/animais\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, valloader, criterion, optimizer, device, num_epochs=5):\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'train_accuracies': [],\n",
    "        'val_accuracies': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in tqdm(enumerate(trainloader, 0), total=len(trainloader)):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100 * correct / total\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['train_accuracies'].append(train_acc)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.2f}%')\n",
    "        \n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "        history['val_losses'].append(val_loss)\n",
    "        history['val_accuracies'].append(val_acc)\n",
    "        print(f'Epoch {epoch+1}, Val Loss: {val_loss:.3f}, Val Accuracy: {val_acc:.2f}%')\n",
    "\n",
    "    print('Treinamento concluído')\n",
    "    return history\n",
    "\n",
    "\n",
    "def test_model(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Acurácia da rede na base de teste: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax1.plot(history['train_losses'], label='Train')\n",
    "    ax1.plot(history['val_losses'], label='Validation')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history['train_accuracies'], label='Train')\n",
    "    ax2.plot(history['val_accuracies'], label='Validation')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_images(images, titles=None, cols=5):\n",
    "    rows = (len(images) + cols - 1) // cols\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))\n",
    "    for i, img in enumerate(images):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dir = os.path.join(data_dir, 'train')\n",
    "example_set = datasets.ImageFolder(example_dir, transform=None)\n",
    "example_images = [example_set[i][0] for i in random.sample(range(len(example_set)), 5)]\n",
    "\n",
    "# Mostrando imagens originais\n",
    "print(\"Imagens Originais\")\n",
    "show_images(example_images)\n",
    "\n",
    "# Exemplo de RandomResizedCrop\n",
    "cropped_images = [transforms.RandomResizedCrop(224)(img) for img in example_images]\n",
    "print(\"Após RandomResizedCrop\")\n",
    "show_images(cropped_images)\n",
    "\n",
    "# Exemplo de RandomHorizontalFlip\n",
    "flipped_images = [transforms.RandomHorizontalFlip()(img) for img in cropped_images]\n",
    "print(\"Após RandomHorizontalFlip\")\n",
    "show_images(flipped_images)\n",
    "\n",
    "# Exemplo de RandomRotation\n",
    "rotated_images = [transforms.RandomRotation(15)(img) for img in example_images]\n",
    "print(\"Após RandomRotation\")\n",
    "show_images(rotated_images)\n",
    "\n",
    "# Exemplo de RandomAffine\n",
    "affined_images = [transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1))(img) for img in example_images]\n",
    "print(\"Após RandomAffine\")\n",
    "show_images(affined_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os datasets\n",
    "transform_aug = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# transform_aug = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "#     transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "transform_no_aug = transforms.Compose([\n",
    "    transforms.Resize(230),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_set_no_aug = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform_no_aug)\n",
    "train_set_aug = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform_aug)\n",
    "val_set = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform_no_aug)\n",
    "\n",
    "train_loader_no_aug = DataLoader(train_set_no_aug, batch_size=32, shuffle=True)\n",
    "train_loader_aug = DataLoader(train_set_aug, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=5, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 2 * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2, padding=0)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2, padding=0)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2, stride=2, padding=0)\n",
    "        x = x.view(-1, 32 * 2 * 2)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNN(num_classes=2).to(device)\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224).to(device)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_aug = CNN(num_classes=2).to(device)\n",
    "\n",
    "criterion_no_aug = nn.CrossEntropyLoss()\n",
    "optimizer_no_aug = optim.SGD(model_no_aug.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "history_no_aug = train_model(model_no_aug, train_loader_no_aug, val_loader, criterion_no_aug, optimizer_no_aug, device, num_epochs=50)\n",
    "\n",
    "plot_history(history_no_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug = CNN(num_classes=2).to(device)\n",
    "\n",
    "criterion_aug = nn.CrossEntropyLoss()\n",
    "optimizer_aug = optim.SGD(model_aug.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "history_aug = train_model(model_aug, train_loader_aug, val_loader, criterion_aug, optimizer_aug, device, num_epochs=50)\n",
    "\n",
    "plot_history(history_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Quais transformações podem ser adicionadas ainda ao processo de data augmentation deste problema? Entre na documentação em https://pytorch.org/vision/stable/transforms.html e escolha, defina os parâmetros e treine novamente o modelo com no mínimo 3 novas transformações."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
