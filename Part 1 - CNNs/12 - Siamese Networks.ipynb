{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10aa1bc0",
   "metadata": {},
   "source": [
    "## 1 – Configuração de ambiente  \n",
    "Importa bibliotecas, define device e hiperparâmetros centrais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "803ec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, itertools, math\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f04c932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "MARGIN = 1.0\n",
    "PAIRS_PER_EPOCH = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8d724",
   "metadata": {},
   "source": [
    "## 2 – Omniglot  \n",
    "Baixa o conjunto e faz split: 30 alfabetos para treino, resto para teste (zero-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a94ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),          # garante 1 canal\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "root = \"./data/omniglot\"\n",
    "full = datasets.Omniglot(root, download=True, background=True, transform=transform)\n",
    "\n",
    "# background=True traz 30 alfabetos; background=False traz 20 alfabetos extras\n",
    "# Usaremos ambos para ter 50 alfabetos, depois dividimos manualmente\n",
    "extra = datasets.Omniglot(root, download=True, background=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b167092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def split_by_alphabet(ds):\n",
    "    alpha2imgs = defaultdict(list)\n",
    "    for img, target in ds:\n",
    "        alpha = ds._characters[target].split(\"/\")[0]\n",
    "        alpha2imgs[alpha].append((img, target))\n",
    "    return alpha2imgs\n",
    "\n",
    "train_alpha = split_by_alphabet(full)          # 30 alfabetos\n",
    "test_alpha  = split_by_alphabet(extra)         # 20 alfabetos (unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c393dae",
   "metadata": {},
   "source": [
    "## 3 – PairDataset  \n",
    "Gera pares (img1, img2, label) on-the-fly: label = 0 positivos, 1 negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3250f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, alpha2imgs, pairs_per_epoch=5000):\n",
    "        self.alpha2imgs = alpha2imgs\n",
    "        self.classes = list(itertools.chain(*[[(a, i) for i in range(len(v))]\n",
    "                                              for a, v in alpha2imgs.items()]))\n",
    "        self.pairs_per_epoch = pairs_per_epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pairs_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        same = random.randint(0, 1) == 0\n",
    "        if same:\n",
    "            alpha = random.choice(list(self.alpha2imgs.keys()))\n",
    "            imgs = random.sample(self.alpha2imgs[alpha], 2)\n",
    "        else:\n",
    "            alpha1, alpha2 = random.sample(list(self.alpha2imgs.keys()), 2)\n",
    "            imgs = [random.choice(self.alpha2imgs[alpha1]),\n",
    "                    random.choice(self.alpha2imgs[alpha2])]\n",
    "        (img1, _), (img2, _) = imgs\n",
    "        label = torch.tensor(float(not same), dtype=torch.float32)  # 0=sim,1=dif\n",
    "        return img1, img2, label\n",
    "\n",
    "dataset = PairDataset(train_alpha, pairs_per_epoch=PAIRS_PER_EPOCH)\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39526ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fabad",
   "metadata": {},
   "source": [
    "## 4 – Rede Siamese  \n",
    "Encoder convolucional → vetor de 4096 dim, compartilhado nos dois ramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e3238a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ↓--- CNN corrigida ---↓\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 10), nn.ReLU(), nn.MaxPool2d(2),   # 64→55→27\n",
    "            nn.Conv2d(64, 128, 7), nn.ReLU(), nn.MaxPool2d(2),   # 27→21→10\n",
    "            nn.Conv2d(128,128, 4), nn.ReLU(), nn.MaxPool2d(2),  # 10→7→3\n",
    "            # ❶ kernel 4 dava erro (3 × 3); reduza para 3 e use padding=1\n",
    "            nn.Conv2d(128,256, 3, padding=1), nn.ReLU(),        # 3→3\n",
    "            # ❷ adapta qualquer tamanho de entrada para 1 × 1\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # 256 canais × 1 × 1  → 4096\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 4096),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.forward_once(x1), self.forward_once(x2)\n",
    "\n",
    "model = SiameseNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef093d18",
   "metadata": {},
   "source": [
    "## 5 – Contrastive Loss + Treino  \n",
    "Minimiza distância euclidiana para pares iguais, empurra para MARGIN para diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17adad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, out1, out2, label):\n",
    "        d = (out1 - out2).pow(2).sum(1).sqrt()\n",
    "        loss = (1 - label) * d.pow(2) + label * torch.clamp(self.margin - d, min=0).pow(2)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "296f122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2819024/2510548251.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Loss 0.3554\n",
      "Epoch 2/10  Loss 0.2545\n",
      "Epoch 3/10  Loss 0.2452\n",
      "Epoch 4/10  Loss 0.2392\n",
      "Epoch 5/10  Loss 0.2401\n",
      "Epoch 6/10  Loss 0.2377\n",
      "Epoch 7/10  Loss 0.2326\n",
      "Epoch 8/10  Loss 0.2358\n",
      "Epoch 9/10  Loss 0.2274\n",
      "Epoch 10/10  Loss 0.2300\n"
     ]
    }
   ],
   "source": [
    "criterion = ContrastiveLoss(MARGIN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    cum_loss = 0.0\n",
    "    for img1, img2, label in train_loader:\n",
    "        img1, img2, label = (\n",
    "            img1.to(device, non_blocking=True),\n",
    "            img2.to(device, non_blocking=True),\n",
    "            label.to(device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out1, out2 = model(img1, img2)\n",
    "            loss = criterion(out1, out2, label)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}  Loss {cum_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2e68c",
   "metadata": {},
   "source": [
    "## 6 – Gerador de tarefas N-way K-shot  \n",
    "Seleciona N classes never-seen, devolve suporte e consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e46cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task(alpha2imgs, N=20, K=1, Q=5):\n",
    "    classes = random.sample(list(alpha2imgs.keys()), N)\n",
    "    support, query, labels = [], [], []\n",
    "    for idx, c in enumerate(classes):\n",
    "        imgs = random.sample(alpha2imgs[c], K + Q)\n",
    "        s, q = imgs[:K], imgs[K:]\n",
    "        support += [(img, idx) for img, _ in s]\n",
    "        query   += [(img, idx) for img, _ in q]\n",
    "    return support, query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d2ca0",
   "metadata": {},
   "source": [
    "## 7 – Avaliação: 20-way 1-shot em alfabetos não vistos  \n",
    "Classifica cada consulta pelo NN mais próximo no set de suporte. Mede acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22c385b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-way 1-shot accuracy: 59.37%\n"
     ]
    }
   ],
   "source": [
    "def embed(imgs):\n",
    "    with torch.no_grad():\n",
    "        return model.forward_once(torch.stack([i for i, _ in imgs]).to(device)).cpu()\n",
    "\n",
    "def few_shot_accuracy(tasks=200, N=20, K=1, Q=5):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    for _ in range(tasks):\n",
    "        support, query = sample_task(test_alpha, N, K, Q)\n",
    "        sup_emb = embed(support)\n",
    "        qry_emb = embed(query)\n",
    "        sup_labels = torch.tensor([lbl for _, lbl in support])\n",
    "        for i, (q_img, gt) in enumerate(query):\n",
    "            dists = torch.norm(qry_emb[i] - sup_emb, dim=1)\n",
    "            pred = sup_labels[dists.argmin()].item()\n",
    "            correct += int(pred == gt)\n",
    "            total += 1\n",
    "    return correct / total\n",
    "\n",
    "acc = few_shot_accuracy(N=3, K=5)\n",
    "print(f\"20-way 1-shot accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d55c26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distância euclidiana: 0.3382\n",
      "Mesmo caractere? Sim\n"
     ]
    }
   ],
   "source": [
    "# Selecionar novo par aleatório\n",
    "img1, img2, label = pair_dataset[42]\n",
    "\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img1.squeeze() * 0.5 + 0.5, cmap='gray')\n",
    "plt.title(\"Imagem 1\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img2.squeeze() * 0.5 + 0.5, cmap='gray')\n",
    "plt.title(\"Imagem 2\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Embeddings e distância\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb1, emb2 = model(img1.unsqueeze(0).to(device), img2.unsqueeze(0).to(device))\n",
    "    dist = F.pairwise_distance(emb1, emb2).item()\n",
    "\n",
    "print(f\"Distância euclidiana: {dist:.4f}\")\n",
    "print(f\"Mesmo caractere? {'Sim' if label.item() == 1 else 'Não'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa51f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
